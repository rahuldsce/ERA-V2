# Backpropogation Manual Calculation


<img width="1629" alt="back_propogation" src="https://github.com/rahuldsce/ERA-V2/assets/7678352/1e6bc8b1-e5ca-448a-b218-9680950089a0">

This Excel example demonstrates how to minimize loss to achieve matching input and target output values. It represents a fully connected layer with 2 inputs and 8 weights. Weight calculation utilizes backpropagation by determining the partial derivative of each weight with respect to the total loss.


This simple two-input problem demonstrates that a higher learning rate leads to faster loss reduction. A lower learning rate requires more training epochs (iterations) to approach zero loss.

Learning rate 0.1
<img width="360" alt="Screenshot 2024-03-02 at 02 03 10" src="https://github.com/rahuldsce/ERA-V2/assets/7678352/9d1adee2-11ef-4d7a-9083-8ae90674b1e7">

Learning rate 0.2
<img width="362" alt="Screenshot 2024-03-02 at 02 03 01" src="https://github.com/rahuldsce/ERA-V2/assets/7678352/261f175f-7a33-4c35-bbaa-300a5812a6cc">

Learning rate 0.5
<img width="364" alt="Screenshot 2024-03-02 at 02 02 53" src="https://github.com/rahuldsce/ERA-V2/assets/7678352/93149f38-60c6-401f-a664-5254370f7573">

Learning rate 0.8
<img width="361" alt="Screenshot 2024-03-02 at 02 02 45" src="https://github.com/rahuldsce/ERA-V2/assets/7678352/ba7b0778-f89f-40a3-9e06-32942498f05f">

Learning rate 1.0
<img width="363" alt="Screenshot 2024-03-02 at 02 02 36" src="https://github.com/rahuldsce/ERA-V2/assets/7678352/09486605-1c85-43ae-9733-f53b29a85446">

Learning rate 2.0
<img width="363" alt="Screenshot 2024-03-02 at 02 02 27" src="https://github.com/rahuldsce/ERA-V2/assets/7678352/4e8f20b5-6452-4712-9877-d82b12634f03">
